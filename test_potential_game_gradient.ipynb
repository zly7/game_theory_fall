{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tg import potential_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 8.5 Game parameters theta tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "Loss 7.531256198883057 Game parameters theta tensor([0.9500, 1.0500, 1.0500, 0.9500], requires_grad=True)\n",
      "Loss 6.627073287963867 Game parameters theta tensor([0.9000, 1.0998, 1.0997, 0.9001], requires_grad=True)\n",
      "Loss 5.7882232666015625 Game parameters theta tensor([0.8500, 1.1493, 1.1489, 0.8502], requires_grad=True)\n",
      "Loss 5.015007972717285 Game parameters theta tensor([0.8001, 1.1982, 1.1973, 0.8006], requires_grad=True)\n",
      "Loss 4.307320594787598 Game parameters theta tensor([0.7502, 1.2465, 1.2446, 0.7513], requires_grad=True)\n",
      "Loss 3.6646108627319336 Game parameters theta tensor([0.7004, 1.2939, 1.2906, 0.7025], requires_grad=True)\n",
      "Loss 3.0858826637268066 Game parameters theta tensor([0.6507, 1.3404, 1.3349, 0.6544], requires_grad=True)\n",
      "Loss 2.5697994232177734 Game parameters theta tensor([0.6012, 1.3856, 1.3774, 0.6071], requires_grad=True)\n",
      "Loss 2.1145811080932617 Game parameters theta tensor([0.5520, 1.4295, 1.4177, 0.5609], requires_grad=True)\n",
      "Loss 1.7180354595184326 Game parameters theta tensor([0.5031, 1.4718, 1.4556, 0.5163], requires_grad=True)\n",
      "Loss 1.3775032758712769 Game parameters theta tensor([0.4546, 1.5125, 1.4909, 0.4736], requires_grad=True)\n",
      "Loss 1.0897986888885498 Game parameters theta tensor([0.4067, 1.5513, 1.5235, 0.4332], requires_grad=True)\n",
      "Loss 0.851126492023468 Game parameters theta tensor([0.3593, 1.5882, 1.5533, 0.3957], requires_grad=True)\n",
      "Loss 0.6570842862129211 Game parameters theta tensor([0.3127, 1.6231, 1.5803, 0.3617], requires_grad=True)\n",
      "Loss 0.5026970505714417 Game parameters theta tensor([0.2670, 1.6558, 1.6045, 0.3317], requires_grad=True)\n",
      "Loss 0.382648766040802 Game parameters theta tensor([0.2225, 1.6864, 1.6260, 0.3062], requires_grad=True)\n",
      "Loss 0.29158440232276917 Game parameters theta tensor([0.1792, 1.7148, 1.6449, 0.2856], requires_grad=True)\n",
      "Loss 0.2245284914970398 Game parameters theta tensor([0.1375, 1.7410, 1.6613, 0.2701], requires_grad=True)\n",
      "Loss 0.17719519138336182 Game parameters theta tensor([0.0975, 1.7651, 1.6754, 0.2598], requires_grad=True)\n",
      "Loss 0.14500975608825684 Game parameters theta tensor([0.0597, 1.7872, 1.6875, 0.2544], requires_grad=True)\n",
      "Loss 0.12664440274238586 Game parameters theta tensor([0.0243, 1.8073, 1.6975, 0.2537], requires_grad=True)\n",
      "Loss 0.11916115880012512 Game parameters theta tensor([-0.0084,  1.8256,  1.7058,  0.2573], requires_grad=True)\n",
      "Loss 0.12030892819166183 Game parameters theta tensor([-0.0379,  1.8423,  1.7125,  0.2645], requires_grad=True)\n",
      "Loss 0.12770503759384155 Game parameters theta tensor([-0.0641,  1.8575,  1.7178,  0.2748], requires_grad=True)\n",
      "Loss 0.13875819742679596 Game parameters theta tensor([-0.0866,  1.8714,  1.7218,  0.2878], requires_grad=True)\n",
      "Loss 0.15183064341545105 Game parameters theta tensor([-0.1053,  1.8843,  1.7246,  0.3027], requires_grad=True)\n",
      "Loss 0.1624900996685028 Game parameters theta tensor([-0.1200,  1.8962,  1.7264,  0.3191], requires_grad=True)\n",
      "Loss 0.16944068670272827 Game parameters theta tensor([-0.1307,  1.9073,  1.7274,  0.3364], requires_grad=True)\n",
      "Loss 0.17131179571151733 Game parameters theta tensor([-0.1375,  1.9177,  1.7277,  0.3543], requires_grad=True)\n",
      "Loss 0.16758190095424652 Game parameters theta tensor([-0.1405,  1.9276,  1.7276,  0.3721], requires_grad=True)\n",
      "Loss 0.15854224562644958 Game parameters theta tensor([-0.1402,  1.9369,  1.7270,  0.3897], requires_grad=True)\n",
      "Loss 0.1451222449541092 Game parameters theta tensor([-0.1368,  1.9458,  1.7262,  0.4065], requires_grad=True)\n",
      "Loss 0.12864214181900024 Game parameters theta tensor([-0.1307,  1.9543,  1.7252,  0.4223], requires_grad=True)\n",
      "Loss 0.1105571836233139 Game parameters theta tensor([-0.1223,  1.9623,  1.7243,  0.4369], requires_grad=True)\n",
      "Loss 0.09224263578653336 Game parameters theta tensor([-0.1121,  1.9699,  1.7236,  0.4501], requires_grad=True)\n",
      "Loss 0.07484447956085205 Game parameters theta tensor([-0.1005,  1.9771,  1.7230,  0.4617], requires_grad=True)\n",
      "Loss 0.05920225754380226 Game parameters theta tensor([-0.0878,  1.9837,  1.7227,  0.4717], requires_grad=True)\n",
      "Loss 0.04583270475268364 Game parameters theta tensor([-0.0745,  1.9899,  1.7228,  0.4799], requires_grad=True)\n",
      "Loss 0.034958429634571075 Game parameters theta tensor([-0.0610,  1.9957,  1.7233,  0.4865], requires_grad=True)\n",
      "Loss 0.026560921221971512 Game parameters theta tensor([-0.0474,  2.0010,  1.7242,  0.4913], requires_grad=True)\n",
      "Loss 0.020446132868528366 Game parameters theta tensor([-0.0342,  2.0058,  1.7255,  0.4945], requires_grad=True)\n",
      "Loss 0.016581127420067787 Game parameters theta tensor([-0.0215,  2.0103,  1.7272,  0.4961], requires_grad=True)\n",
      "Loss 0.014014464803040028 Game parameters theta tensor([-0.0096,  2.0143,  1.7294,  0.4963], requires_grad=True)\n",
      "Loss 0.01268160529434681 Game parameters theta tensor([1.4918e-03, 2.0180e+00, 1.7319e+00, 4.9514e-01], requires_grad=True)\n",
      "Loss 0.012236533686518669 Game parameters theta tensor([0.0115, 2.0213, 1.7347, 0.4928], requires_grad=True)\n",
      "Loss 0.012373006902635098 Game parameters theta tensor([0.0204, 2.0244, 1.7378, 0.4894], requires_grad=True)\n",
      "Loss 0.012835781089961529 Game parameters theta tensor([0.0281, 2.0272, 1.7410, 0.4852], requires_grad=True)\n",
      "Loss 0.013423128053545952 Game parameters theta tensor([0.0346, 2.0297, 1.7445, 0.4803], requires_grad=True)\n",
      "Loss 0.013984921388328075 Game parameters theta tensor([0.0399, 2.0320, 1.7479, 0.4749], requires_grad=True)\n",
      "Loss 0.014416570775210857 Game parameters theta tensor([0.0440, 2.0342, 1.7515, 0.4692], requires_grad=True)\n",
      "Loss 0.014651698060333729 Game parameters theta tensor([0.0469, 2.0361, 1.7549, 0.4632], requires_grad=True)\n",
      "Loss 0.014654348604381084 Game parameters theta tensor([0.0487, 2.0380, 1.7583, 0.4573], requires_grad=True)\n",
      "Loss 0.014411688782274723 Game parameters theta tensor([0.0494, 2.0396, 1.7615, 0.4515], requires_grad=True)\n",
      "Loss 0.013928213156759739 Game parameters theta tensor([0.0492, 2.0412, 1.7645, 0.4460], requires_grad=True)\n",
      "Loss 0.013221112079918385 Game parameters theta tensor([0.0481, 2.0427, 1.7673, 0.4408], requires_grad=True)\n",
      "Loss 0.01231714803725481 Game parameters theta tensor([0.0461, 2.0440, 1.7698, 0.4362], requires_grad=True)\n",
      "Loss 0.011250324547290802 Game parameters theta tensor([0.0435, 2.0453, 1.7721, 0.4321], requires_grad=True)\n",
      "Loss 0.01006055623292923 Game parameters theta tensor([0.0403, 2.0465, 1.7740, 0.4287], requires_grad=True)\n",
      "Loss 0.008791975677013397 Game parameters theta tensor([0.0367, 2.0476, 1.7756, 0.4260], requires_grad=True)\n",
      "Loss 0.00749184237793088 Game parameters theta tensor([0.0326, 2.0486, 1.7769, 0.4239], requires_grad=True)\n",
      "Loss 0.006208325736224651 Game parameters theta tensor([0.0283, 2.0496, 1.7780, 0.4226], requires_grad=True)\n",
      "Loss 0.004988097585737705 Game parameters theta tensor([0.0238, 2.0505, 1.7787, 0.4219], requires_grad=True)\n",
      "Loss 0.003873661858960986 Game parameters theta tensor([0.0192, 2.0513, 1.7791, 0.4219], requires_grad=True)\n",
      "Loss 0.0029005310498178005 Game parameters theta tensor([0.0147, 2.0520, 1.7793, 0.4225], requires_grad=True)\n",
      "Loss 0.0020945915021002293 Game parameters theta tensor([0.0103, 2.0527, 1.7793, 0.4236], requires_grad=True)\n",
      "Loss 0.001470426912419498 Game parameters theta tensor([0.0060, 2.0533, 1.7791, 0.4251], requires_grad=True)\n",
      "Loss 0.0010303149465471506 Game parameters theta tensor([0.0021, 2.0538, 1.7787, 0.4270], requires_grad=True)\n",
      "Loss 0.0007644432480446994 Game parameters theta tensor([-1.5542e-03,  2.0543e+00,  1.7782e+00,  4.2926e-01],\n",
      "       requires_grad=True)\n",
      "Loss 0.0006523806368932128 Game parameters theta tensor([-0.0048,  2.0548,  1.7776,  0.4317], requires_grad=True)\n",
      "Loss 0.000665282947011292 Game parameters theta tensor([-0.0077,  2.0552,  1.7770,  0.4342], requires_grad=True)\n",
      "Loss 0.0007689511985518038 Game parameters theta tensor([-0.0101,  2.0556,  1.7763,  0.4367], requires_grad=True)\n",
      "Loss 0.0009272035676985979 Game parameters theta tensor([-0.0121,  2.0559,  1.7756,  0.4392], requires_grad=True)\n",
      "Loss 0.0011051477631554008 Game parameters theta tensor([-0.0136,  2.0562,  1.7749,  0.4416], requires_grad=True)\n",
      "Loss 0.0012720099184662104 Game parameters theta tensor([-0.0146,  2.0565,  1.7742,  0.4438], requires_grad=True)\n",
      "Loss 0.0014034113846719265 Game parameters theta tensor([-0.0152,  2.0568,  1.7737,  0.4458], requires_grad=True)\n",
      "Loss 0.0014825711259618402 Game parameters theta tensor([-0.0154,  2.0570,  1.7732,  0.4475], requires_grad=True)\n",
      "Loss 0.001500902697443962 Game parameters theta tensor([-0.0152,  2.0572,  1.7728,  0.4490], requires_grad=True)\n",
      "Loss 0.0014575282111763954 Game parameters theta tensor([-0.0147,  2.0575,  1.7725,  0.4501], requires_grad=True)\n",
      "Loss 0.0013582257088273764 Game parameters theta tensor([-0.0139,  2.0576,  1.7723,  0.4510], requires_grad=True)\n",
      "Loss 0.0012137906160205603 Game parameters theta tensor([-0.0128,  2.0578,  1.7722,  0.4515], requires_grad=True)\n",
      "Loss 0.0010383683256804943 Game parameters theta tensor([-0.0115,  2.0580,  1.7722,  0.4517], requires_grad=True)\n",
      "Loss 0.0008474522037431598 Game parameters theta tensor([-0.0100,  2.0581,  1.7723,  0.4516], requires_grad=True)\n",
      "Loss 0.0006562003400176764 Game parameters theta tensor([-0.0085,  2.0583,  1.7725,  0.4513], requires_grad=True)\n",
      "Loss 0.00047796970466151834 Game parameters theta tensor([-0.0069,  2.0584,  1.7728,  0.4508], requires_grad=True)\n",
      "Loss 0.00032333959825336933 Game parameters theta tensor([-0.0052,  2.0585,  1.7731,  0.4501], requires_grad=True)\n",
      "Loss 0.00019940410857088864 Game parameters theta tensor([-0.0036,  2.0586,  1.7735,  0.4493], requires_grad=True)\n",
      "Loss 0.00010963980457745492 Game parameters theta tensor([-0.0021,  2.0587,  1.7739,  0.4484], requires_grad=True)\n",
      "Loss 5.402338138082996e-05 Game parameters theta tensor([-6.3853e-04,  2.0588e+00,  1.7743e+00,  4.4736e-01],\n",
      "       requires_grad=True)\n",
      "Loss 2.9561408155132085e-05 Game parameters theta tensor([6.8623e-04, 2.0589e+00, 1.7748e+00, 4.4634e-01], requires_grad=True)\n",
      "Loss 3.097509761573747e-05 Game parameters theta tensor([1.8675e-03, 2.0590e+00, 1.7752e+00, 4.4533e-01], requires_grad=True)\n",
      "Loss 5.152368976268917e-05 Game parameters theta tensor([0.0029, 2.0590, 1.7756, 0.4444], requires_grad=True)\n",
      "Loss 8.386360423173755e-05 Game parameters theta tensor([0.0037, 2.0591, 1.7759, 0.4435], requires_grad=True)\n",
      "Loss 0.00012083324691047892 Game parameters theta tensor([0.0044, 2.0591, 1.7763, 0.4427], requires_grad=True)\n",
      "Loss 0.000156126290676184 Game parameters theta tensor([0.0049, 2.0592, 1.7766, 0.4420], requires_grad=True)\n",
      "Loss 0.00018475326942279935 Game parameters theta tensor([0.0052, 2.0592, 1.7768, 0.4414], requires_grad=True)\n",
      "Loss 0.00020335696171969175 Game parameters theta tensor([0.0054, 2.0593, 1.7770, 0.4410], requires_grad=True)\n",
      "Loss 0.00021031044889241457 Game parameters theta tensor([0.0054, 2.0593, 1.7771, 0.4407], requires_grad=True)\n",
      "Loss 0.0002055551449302584 Game parameters theta tensor([0.0053, 2.0594, 1.7772, 0.4405], requires_grad=True)\n",
      "Loss 0.00019040486949961632 Game parameters theta tensor([0.0050, 2.0594, 1.7772, 0.4405], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "def potential1(theta, sv, x, other_params = None):\n",
    "    # potential1 pulls the solution vector x with a quadratic potential towards x[:]=0\n",
    "    # the weight of the potentials is given by theta[0] and theta[2]\n",
    "    return -(theta[0]*(x[0])**2 + theta[2]*(x[1])**2).mean()\n",
    "\n",
    "def potential2(theta, sv, x, other_params = None):\n",
    "    # potential2 pulls the solution vector x with a quadratic potential towards x[:]=5\n",
    "    # the weight of the potentials is given by theta[1] and theta[3]\n",
    "    return  -(theta[1]*(5-x[0])**2 + theta[3]*(5-x[1])**2).mean()\n",
    "\n",
    "# construct potential game with convex_ndims = 2 (the dimensionality of the x vector) and the respective player utilities\n",
    "game = potential_game.PotGame( convex_ndims = 2, player_utilities = [potential1,potential2])\n",
    "\n",
    "# create and initialize theta game parameters vector\n",
    "theta = torch.tensor([1,1,1,1]).float()\n",
    "theta.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam([theta], lr=5e-2)\n",
    "\n",
    "\n",
    "desired_y = torch.tensor([5.,1]).float()\n",
    "\n",
    "def closure():\n",
    "    # Forward pass: compute predicted y (solution of game with given theta)\n",
    "    sol = game.solve(theta, None)\n",
    "    y = sol[2]\n",
    "    \n",
    "    # Compute loss with respect to desired y.\n",
    "    loss = torch.sum((y[-2:]-desired_y)**2)\n",
    "\n",
    "    print(\"Loss\", loss.item(), \"Game parameters theta\", theta)\n",
    "\n",
    "    # do gradient step to adjust game parameters such that game solution y is closer to desired y\n",
    "    optimizer.zero_grad() # do zero_grad before backward pass, the inner loop of the game solver also accumulates gradients which have to be cleared\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "for t in range(100):\n",
    "    optimizer.step(closure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coding",
   "language": "python",
   "name": "coding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
